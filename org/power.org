#+TITLE: Руководство пользователя кластера OpenPOWER ВЦ ДВО РАН
#+OPTIONS: toc:t H:6 num:t html-postamble:nil ^:nil tags:nil author:nil
#+SETUPFILE: theme-readtheorg-local.setup
#+HTML_HEAD: <style type="text/css">.org-src-name{ text-align: right; }</style>
#+HTML_HEAD: <style type="text/css">.outline-2{ margin-top: 60px; }</style>

#+begin_verse
Версия: src_emacs-lisp[:results raw]{"*0.1*"}
Дата: src_emacs-lisp[:results raw]{(format-time-string "*%d.%m.%Y*")}
#+end_verse

\\
\\
Данное руководство содержит минимально необходимый объем информации
для работы на кластере ВЦ ДВО РАН: описание процесса регистрации,
сведения по работе в ОС Linux (вход в систему, работа с каталогами и
файлами, мониторинг) и работе с MPI программами и не параллельными
программами на кластере (компиляция, запуск, остановка, работа с
очередями). В тексте под термином параллельная программа
подразумеваются только MPI программы.

Команды и переменные командного
интерпретатора, названия программ, листинги, непосредственный
ввод/вывод консоли выделены моноширинным шрифтом.

Вопросы относительно работы кластера следует отправлять на /e-mail/:
[[mailto:support@hpc.febras.net][support@hpc.febras.net]].

Вопросы относительно этого документа (ошибки, неточности, предложения)
можно отправлять на /e-mail/: [[mailto:support@hpc.febras.net][support@hpc.febras.net]].

* Содержание :ignore:

#+TOC: headlines 3 local

* Регистрация
:PROPERTIES:
:CUSTOM_ID: registration
:END:

Регистрация пользователей на кластере происходит через систему ЦКП
http://ckp.ccfebras.ru.

* Вход в систему
:PROPERTIES:
:CUSTOM_ID: login
:END:

Для работы с системой пользователь должен иметь свою учетную запись на
управляющем узле кластера. Регистрация пользователя на кластере
происходит в соответствии с предыдущей частью руководства. После
регистрации пользователь получает свое имя (логин), пароль и домашнюю
директорию. Если имя пользователя, например, будет *user*, то домашняя
папка находится в */home/user*.

При первом входе в систему предлагается сменить пароль. Требования к
новому паролю: он должен быть достаточной длины, содержать хотя бы 1
цифру и 1 заглавную букву. Директория =~/.ssh= содержит пару ключей
(/id_rsa/ и /authorized_keys/) для доступа к узлам кластера. При
добавлении личного ssh-ключа, необходимо добавить открытый ключ в файл
/authorized_keys/ через перенос строки (т.е. после уже имеющегося одного
открытого ключа).

Пользователи имеют возможность работать на кластере с любой машины,
находящейся в сети института и интернет. Для входа в систему
пользователю необходим адрес сервера ([[http://jupiter.febras.net][jupiter.febras.net]]), а также
имя и пароль, полученные при регистрации.

** Вход с Windows-машины
:PROPERTIES:
:CUSTOM_ID: login_windows
:END:

Работа с системой осуществляется по безопасному протоколу SSH при помощи какого-либо
ssh-клиента. Клиент должен поддерживать протокол версии 2. Рекомендуется использовать 
[[https://www.chiark.greenend.org.uk/~sgtatham/putty/][PuTTY]].
Эта программа является свободно распространяемой и проста в использовании.

После запуска программы (рис. 1) пользователь должен выбрать протокол
ssh и в поле «Host Name (or IP address)» указать адрес
сервера. Нажатие на «Open» приведет к отправке запроса на
подключение. В случае успешного подключения к серверу будет предложено
ввести имя (логин), а затем и пароль.

#+NAME: fig:putty
#+caption: Окно ssh-клиента PuTTY
[[./putty.png]]

При вводе пароля символы на экране не отображаются. Если все введено
правильно, то пользователь автоматически окажется в своей домашней
директории. Этот каталог доступен пользователю с любого узла кластера.

#+begin_note
*Примечание*. На кластере существует единое дисковое пространство для
директорий */opt* (только чтение) и */home*.  Все узлы используют
дисковый массив сервера посредством сетевой файловой системы NFS. Файл
записанный на одном из узлов кластера автоматически становится
доступен на любом другом.
#+end_note

Работа в ssh-сессии происходит в терминальном (текстовом, консольном)
режиме. Необходимо помнить, что консоль Linux, в отличии от Windows,
различает регистр вводимых символов, то есть =mydoc.txt= и =mydoc.TXT= не
одно и то же. После входа на экране отображается консоль командного
интерпретатора в следующем формате имя_пользователя@машина текущий_каталог:

#+begin_src raw
[user@jupiter ~]$
#+end_src

** Вход с терминала Linux
:PROPERTIES:
:CUSTOM_ID: login_linux
:END:

В любой дистрибутив ОС Linux входит терминальный ssh-клиент (обычно
OpenSSH). Минимальный формат команды для подключения к кластеру таков:

#+begin_src raw
[user@localhost ~]$ ssh jupiter.febras.net -l имя_пользователя
#+end_src

* Копирование файлов
:PROPERTIES:
:CUSTOM_ID: copy_files
:END:

См. соответсвующий раздел [[http://lits.ccfebras.ru/assets/files/user_guide.pdf][руководства кластера версии 5.0]].

* Работа на кластере
:PROPERTIES:
:CUSTOM_ID: work_cluster
:END:

** Навигация
:PROPERTIES:
:CUSTOM_ID: navigation
:END:

См. соответсвующий раздел [[http://lits.ccfebras.ru/assets/files/user_guide.pdf][руководства кластера версии 5.0]].

** Редактирование файлов
:PROPERTIES:
:CUSTOM_ID: editing
:END:

См. соответсвующий раздел [[http://lits.ccfebras.ru/assets/files/user_guide.pdf][руководства кластера версии 5.0]].

** Компиляция программ
:PROPERTIES:
:CUSTOM_ID: compilation
:END:

На кластере (на src_emacs-lisp[:results raw]{(format-time-string "%d.%m.%Y")})
 поддерживаются следующие компиляторы языков
программирования для архитектуры ppc64le:

#+ATTR_HTML: :align center 
#+caption: Компиляторы на кластере
| Компилятор            | Путь к файлу компилятора | Язык       |
| GNU C 4.8.5           | /usr/bin/gcc             | C          |
| GNU C++ 4.8.5         | /usr/bin/g++             | C++        |
| GNU Fortran 4.8.5     | /usr/bin/gfortran        | Fortran 90 |
| IBM XL Fortran 15.1.5 | /usr/bin/xlf             | Fortran 77 |
| IBM XL Fortran 15.1.5 | /usr/bin/xlf90           | Fortran 90 |
| IBM XL C 13.1.5       | /usr/bin/xlc             | C          |
| IBM XL C++ 13.1.5     | /usr/bin/xlc++           | C++        |
| NVIDIA Cuda 8.0.61    | /usr/local/cuda/bin/nvcc | C/C++      |

Компиляторы GNU и IBM XL находятся в каталоге, доступном для всех
пользователей. Поэтому, например, при вызове команды:
#+begin_src raw
[user@jupiter ~]$ xlc
#+end_src
будет запускаться компилятор IBM XL C 13.1.5. В переменной среды
«LD_LIBRARY_PATH» также указаны пути к библиотекам этих компиляторов.

На кластере используется система модулей окружения ([[http://modules.sourceforge.net/][Environment
Modules]]). Загруженные модули можно посмотреть с помощью команды:
#+begin_src raw
[user@jupiter ~]$ module list
Currently Loaded Modulefiles:
  1) pbs            2) cuda           3) essl           4) spectrum_mpi
#+end_src
Видно, что в отличии от компиляторов GNU и IBM XL, компилятор cuda
подключается при помощи соответствующего модуля. Среди подключенных
по-умолчанию модулей находится и /spectrum_mpi/.

В качестве реализации MPI библиотеки на кластере (на
src_emacs-lisp[:results raw]{(format-time-string "%d.%m.%Y")})
поддерживается IBM Spectrum MPI. Кроме того, можно использовать любую
из имеющихся дополнительных реализаций, подключив необходимый модуль:

#+begin_src raw
module unload spectrum_mpi
module load openmpi/gcc/1.10.6/4.8.5
#+end_src
Все реализациии MPI конфликтуют между собой, поэтому необходимо
предварительно отключать альтернативный модуль. После подключения
нужного модуля, пути к данной библиотеке добавляются в переменные
среды «PATH» и «LD_LIBRARY_PATH».

Список доступных модулей можно увидеть с помощью команды:
#+begin_src raw
[user@jupiter ~]$ module avail

------------------------------------------ /usr/share/Modules/modulefiles -------------------------------------------
dot         module-git  module-info modules     null        use.own

------------------------------------------------- /etc/modulefiles --------------------------------------------------
cuda                      openmpi/gcc/2.1.0/4.8.5   openmpi/xl/2.1.0          spectrum_mpi
essl                      openmpi/pgi/1.10.2/2016   pbs
openmpi/gcc/1.10.6/4.8.5  openmpi/xl/1.10.6         pgi/16.10(default)
openmpi/gcc/2.0.2a1/4.8.5 openmpi/xl/2.0.2a1        pgi/2016
#+end_src

Для компиляции mpi программ лучше всего использовать обёртки к
компиляторам, чем вручную прописывать для этого специальные
флаги. Так, например, чтобы скомпилировать mpi программу, написанную
на языке Fortran, нужно воспользоваться оберткой =mpifort=. Данная
команда вызовет компилятор IBM XL Fortran (при условии, что подключен
модуль spectrum_mpi), с указанием всех необходимых флагов.

Распишем соответствие между обертками и соответствующими им компиляторами:

#+ATTR_HTML: :align center 
#+caption: Соответствие между обертками MPI и компиляторами
| Обертка | XL      | GNU      |
| mpicc   | xlc_r   | gcc      |
| mpifort | xlf_r   | gfortran |
| mpif77  | xlf_r   | gfortran |
| mpif90  | xlf90_r | gfortran |
| mpic++  | xlC_r   | g++      |
| mpicxx  | xlC_r   | g++      |

Для того, чтобы посмотреть какие опции компилятора указываются при
вызове обертки, можно воспользоваться следующей командой:

#+begin_src raw
[user@jupiter ~]$ mpicc -show
gcc -I/opt/soft/openmpi/1.10.6/gcc/include -pthread -Wl,-rpath
-Wl,/opt/soft/openmpi/1.10.6/gcc/lib -Wl,--enable-new-dtags
-L/opt/soft/openmpi/1.10.6/gcc/lib -lmpi
#+end_src

Как видно из данного вывода, единственными опциями, которые может
потребоваться указать при вызове компилятора, могут оказаться опции
оптимизации.

*** Замечания по разработке программ на отдельной машине
:PROPERTIES:
:CUSTOM_ID: notes_dev
:END:

Практически все реализации MPI поддерживают запуск параллельных
приложений в режиме эмуляции на отдельно взятой рабочей станции. Это
можно делать как на Linux, так и Windows машинах.

В Linux рекомендуется использовать пакет OpenMPI, а для создания MPI
приложений на Windows машинах можно использовать пакет MPICH в версии
для Windows. Для успешного портирования программ с Windows на Linux не
следует использовать расширения предоставляемые средами
программирования, такими как Visual Studio и Borland Builder.

Подготовленные исходные коды программ лучше всего компилировать на
кластере.

** Запуск задач
:PROPERTIES:
:CUSTOM_ID: run_tasks
:END:

*** Диспетчеризация задач
:PROPERTIES:
:CUSTOM_ID: dispatch_tasks
:END:

Для диспетчеризации задач на кластере используется система PBS
Pro. С её помощью пользователь может отправлять свои задачи на
исполнение, снимать их с исполнения и получать информацию по текущему
статусу задачи.

Данная система построена на основе очередей, где под очередью
понимается набор пользовательских процессов (программ, задач)
выполняющихся в рамках системы диспетчеризации. Каждой очереди
сопоставлен ряд атрибутов, в зависимости от которых к задаче будут
применены те или иные действия. Типичными атрибутами являются название
(идентификатор) очереди, её приоритет, доступные ресурсы, количество
задач. В общем случае термин очередь не означает, то что программы в
ней будут выполняться строго последовательно.

Чтобы поставить задачу на исполнение, пользователь должен добавить ее
при помощи команды =qsub= в какую-либо очередь. Очереди отличаются
друг от друга совокупностью ресурсов, которыми они обладают.

*** Система очередей
:PROPERTIES:
:CUSTOM_ID: queues
:END:

На данный момент действует 1 очередь: /workq/.

Для получения информации об очередях, можно выполнить команду =qstat -q=:
#+begin_src raw
[eab@jupiter install]$ qstat -q

server: jupiter1

Queue            Memory CPU Time Walltime Node   Run   Que   Lm  State
---------------- ------ -------- -------- ---- ----- ----- ----  -----
workq              --      --       --     --      0     0   --   E R
                                               ----- -----
                                                   0     0

#+end_src

*Queue* – имя очереди; *Run* – число выполняемых задач; *Que* – число задач, ожидающих начала выполнения.
Команда =qstat -Qf имя_очереди=  позволяет получить информацию о конкретной очереди.

*** Постановка задачи в очередь
:PROPERTIES:
:CUSTOM_ID: queue_tasks
:END:

Для постановки задачи в очередь на исполнение используется команда
=qsub=. Данная команда принимает в качестве параметра имя скрипта, в
котором описываются требуемые задачей ресурсы и указываются команды,
исполняемые при запуске. Рассмотрим пример, иллюстрирующий запуск
ранее скомпилированной программы на 1 чанке (некоторой виртуальной
части) кластера, с использованием 4 mpi процессов, выделением 4
процессоров и 1 GPU на этом чанке:

#+begin_src raw
[user@jupiter mpi_test]$ cat mpi_test.qsub
#PBS -k oe
#PBS -l select=1:mpiprocs=4:ncpus=4:ngpus=1
#PBS -l place=shared
#PBS -r n
#PBS -M user@mail.com
#PBS -m abe
#PBS -q workq
#PBS -N mpi_test
#!/bin/sh

cd /home/user/test/mpi_test

module unload spectrum_mpi && module load openmpi/gcc/1.10.6/4.8.5
mpirun ./mpi

exit 0
[user@jupiter mpi_test]$ qsub mpi_test.qsub
66330.jupiter1
#+end_src

Если команда выполнена успешно, то на экране отобразится идентификатор
задачи (в данном случае это /66330.jupiter1/), в противном случае
появится сообщение об ошибке. Ошибки пользовательской программы
(неправильная компиляция и т.п.) проявятся только при переходе задачи
к активному состоянию.

#+begin_note
*Примечание.* Весь вывод программы в стандартный поток и в поток ошибок
перенаправляется в файлы, находящиеся в домашней директории пользователя.
Названия таких файлов имеют формат =имя_задачи.(e/o)порядковый_номер=. Для
запущенной задачи это будут: /mpi_test.e66330/ – для потока ошибок и
/mpi_test.o66330/ – для стандартного потока вывода.
#+end_note

Прокомментируем каждую из строчек скрипта =mpi_test.qsub=:

=#PBS -k oe= — указание сброса потока вывода (o) и потока ошибок (e)\\
=#PBS -l select=1:mpiprocs=4:ncpus=4:ngpus=1= — требуемое количество чанков (1); количество mpi процессов (4), количество выделяемых процессоров (4) и количество выделяемых GPU (1) на каждом чанке\\
=#PBS -l place=shared= — использование узла вместе с другими задачами; *shared* — совместное использование, *excl* — монопольное использование\\
=#PBS -r n= — является ли задача перезапускаемой (задачей с контрольными точками);
*y* — является, *n* — не является\\
=#PBS -M user@mail.com= — почтовый адрес пользователя\\
=#PBS -m abe= — какие сообщения отправляются на указанный адрес (*a* — ошибка в
выполнении задачи, *b* — начало выполнения, *e* — завершение
выполнения)\\
=#PBS -q workq= — идентификатор очереди\\
=#PBS -N mpi_test= — название задачи\\
=#!/bin/sh= — указание необходимого командного интерпретатора\\
=cd /home/user/test/mpi_test= — переход в директорию с исполняемым файлом\\
=module unload spectrum_mpi && module load openmpi/gcc/1.10.6/4.8.5= — выбор openmpi вместо spectrum_mpi\\
=mpirun ./mpi= — запуск приложения\\
=exit 0= — выход

*** Запуск интерактивных программ
:PROPERTIES:
:CUSTOM_ID: interactive
:END:

Программы, использующие стандартный ввод, называются
интерактивными. Как правило, такие программы после запуска требуют от
пользователя ввода данных. При постановке задачи в очередь любая
программа переводится в фоновый режим. В этом режиме ввод данных
пользователем в запущенную программу невозможен. Для передачи данных
таким программам используется механизм перенаправления стандартных
потоков ввода/вывода.

Для перенаправления подготавливается текстовый файл, содержимое
которого в точности представляет собой данные, вводимые пользователем.
Например, если программа =solver= предполагает ввод в первой строке
размерности матрицы, а во второй количества итераций, то текстовый
файл =input.txt= будет иметь вид:

#+begin_src raw
[user@jupiter solver]$ cat input.txt
10000000
1000
#+end_src

После каждого числа обязателен символ новой строки. Запуск программы
на выполнение производится так:

#+begin_src raw
solver < input.txt
#+end_src

Скрипт для постановки в очередь задания, в рамках которого будет
выполняться интерактивная программа, будет выглядеть следующим
образом:

#+begin_src raw
[user@jupiter solver]$ cat job.qsub
#PBS -k oe
#PBS -l select=1:ncpus=8:mpiprocs=8
#PBS -r n
#PBS -M user@mail.com
#PBS -m abe
#PBS -q workq
#PBS -N solver
#!/bin/sh

cd /home/user/test/solver

mpirun ./solver < ./input.txt

exit 0
#+end_src

*** Запуск непараллельных программ
:PROPERTIES:
:CUSTOM_ID: nonparallel
:END:

Запуск непараллельных программ практически ничем не отличается от запуска параллельных
программ. Единственное отличие заключается в том, что в =qsub= скрипте такой программы необходимо
указать, что для её работы необходим только один логический процессор:

#+begin_src raw
#PBS -l select=1:ncpus=1
#+end_src

Также в этом скрипте необходимо запускать непосредственно исполняемый
файл программы, то есть не использовать для запуска =mpiexec=.

*** Состояние пользовательских задач
:PROPERTIES:
:CUSTOM_ID: user_tasks
:END:

Для получения информации об очередях и задачах пользователя
используется команда =qstat=. Выполнение этой команды без параметров
покажет все задачи пользователя и их состояние.

#+begin_src raw
[user@jupiter ~]$ qstat
Job id                  Name             User            Time Use S Queue
----------------------- ---------------- --------------- -------- - -----
700.jupiter1            mpi_test         user            00:10:40 R workq
701.jupiter1            sample_job       user            0        Q workq
702.jupiter1            solver           user            0        Q workq
#+end_src

*Job id* — идентификатор задачи, полученный при выполнении =qsub=;
*Name* — имя задачи; *User* — имя пользователя, запустившего задачу;
*Time Use* — процессорное время, потраченное задачей; *S (State)* — с
остояние задачи ( *R* – задача выполняется, *Q* – ожидает в очереди);
*Queue* — очередь.

В данном случае пользователю *user* принадлежат три задачи.

С помощью команды =qstat -n идентификатор_задачи= можно получить
список узлов, на которых выполняется конкретная задача. Эта информация
полезна при мониторинге эффективности использования вычислительных
ресурсов с использованием системы Ganglia, так как позволяет
отслеживать состояние только используемых задачей узлов.

#+begin_src raw
[user@jupiter ~]$ qstat -n 701

jupiter1:
                                                           Req'd  Req'd   Elap
Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time
--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----
298.jupiter1    eab      workq    mpi_test      --    1   4    2gb 00:00 R   -- 
   jupiter1/0*4
#+end_src

Для получения более подробной информации о конкретной задаче можно запустить команду
=qstat -f идентификатор_задачи=.

*** Остановка задач
:PROPERTIES:
:CUSTOM_ID: stop_tasks
:END:

Остановка программы производится командой =qdel идентификатор_задачи=
#+begin_src raw
[user@jupiter ~]$ qdel 700
#+end_src

Этой командой задача, стоящая в очереди, убирается из нее, а
выполняющаяся задача снимается с выполнения. Следующая по очереди и
приоритету задача встает на выполнение.

Задача снимается в течении некоторого времени, поэтому при вызове
=qstat= непосредственно после =qdel= удаленная задача все еще может
быть отражена в таблице.

* Мониторинг
:PROPERTIES:
:CUSTOM_ID: monitoring
:END:

** Web-интерфейс
:PROPERTIES:
:CUSTOM_ID: web_interface
:END:

Мониторинг кластера реализован при помощи системы Ganglia. Эта система позволяет следить
за ресурсами кластера посредством web-интерфейса. Система мониторинга находится по адресу 
http://jupiter.febras.net/ganglia.

Для мониторинга пользователю доступно большое число типов ресурсов:
загруженность процессора, оперативная память, загрузка сети, средняя
загрузка, количество процессов и ряд других.  Имеется возможность
наблюдать как за всеми узлами в кластере (по одному параметру), так и
за каждым (по всем параметрам).


** Консоль
:PROPERTIES:
:CUSTOM_ID: console
:END:

Кроме графического интерфейса существует несколько полезных консольных
команд для мониторинга. Команда =pbsnodes имя_узла= позволяет
получить информацию о конкретном узле: тип, состояние, количество
процессоров, выполняющиеся задачи. Ниже представлен фрагмент вывода
этой команды.

#+begin_src raw
[user@jupiter ~]$ pbsnodes jupiter2
jupiter2
     Mom = jupiter2
     Port = 15002
     pbs_version = 14.1.0
     ntype = PBS
     state = free
     pcpus = 160
     resources_available.arch = linux
     resources_available.host = jupiter2
     resources_available.mem = 263653568kb
     resources_available.ncpus = 160
     resources_available.ngpus = 2
     resources_available.vnode = jupiter2
     resources_assigned.accelerator_memory = 0kb
     resources_assigned.mem = 0kb
     resources_assigned.naccelerators = 0
     resources_assigned.ncpus = 0
     resources_assigned.netwins = 0
     resources_assigned.ngpus = 0
     resources_assigned.vmem = 0kb
     resv_enable = True
     sharing = default_shared

#+end_src

*state* – состояние узла (*job-exclusive* – все ресурсы узла заняты;
*free* – на узле есть свободные ресурсы для запуска заданий; *offline*
– узел временно выведен из эксплуатации, запуск заданий на нем
невозможен; *down* – узел выключен); *pcpus* – число процессорных
потоков на узле; *jobs* – задачи, запущенные на узле;
*resources_available.ngpus* – число GPU на узле.

При выполнении команды =pbsnodes -a -S -j= будет выведена сводная информация обо
всех узлах кластера.

#+begin_src raw
[user@jupiter ~]$ pbsnodes -a -S -j
                                                        mem       ncpus   nmics   ngpus
vnode           state           njobs   run   susp      f/t        f/t     f/t     f/t   jobs
--------------- --------------- ------ ----- ------ ------------ ------- ------- ------- -------
jupiter1        free                 0     0      0  251gb/251gb 160/160     0/0     2/2 --
jupiter2        free                 0     0      0  251gb/251gb 160/160     0/0     2/2 --
jupiter3        free                 0     0      0  251gb/251gb 160/160     0/0     2/2 --
jupiter4        free                 0     0      0  251gb/251gb 160/160     0/0     2/2 --
jupiter5        free                 0     0      0  251gb/251gb 160/160     0/0     2/2 --

#+end_src

* Справочная информация
:PROPERTIES:
:CUSTOM_ID: reference
:END:

Описание основных команд при работе в ОС Linux – http://wwwinfo.jinr.ru/unixinfo/pc/lin_os.html
Документация к системе диспетчеризации заданий PBS Pro — http://www.pbsworks.com/pdfs/PBSUserGuide14.2.pdf
