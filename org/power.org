#+TITLE: Руководство пользователя кластера OpenPOWER ВЦ ДВО РАН
#+OPTIONS: toc:t H:6 num:nil html-postamble:nil ^:nil tags:nil author:nil
#+SETUPFILE: theme-readtheorg-local.setup
#+HTML_HEAD: <style type="text/css">.org-src-name{ text-align: right; }</style>
#+HTML_HEAD: <style type="text/css">.outline-2{ margin-top: 60px; }</style>

#+begin_verse
Версия: src_emacs-lisp[:results raw]{"*0.1*"}
Дата: src_emacs-lisp[:results raw]{(format-time-string "*%d.%m.%Y*")}
#+end_verse

\\
\\
Данное руководство содержит минимально необходимый объем информации
для работы на кластере ВЦ ДВО РАН: описание процесса регистрации,
сведения по работе в ОС Linux (вход в систему, работа с каталогами и
файлами, мониторинг) и работе с MPI программами и не параллельными
программами на кластере (компиляция, запуск, остановка, работа с
очередями). В тексте под термином параллельная программа
подразумеваются только MPI программы.

Команды и переменные командного
интерпретатора, названия программ, листинги, непосредственный
ввод/вывод консоли выделены моноширинным шрифтом.

Вопросы относительно работы кластера следует отправлять на /e-mail/:
[[mailto:support@hpc.febras.net][support@hpc.febras.net]].

Вопросы относительно этого документа (ошибки, неточности, предложения)
можно отправлять на /e-mail/: [[mailto:support@hpc.febras.net][support@hpc.febras.net]].

* Содержание :ignore:

#+TOC: headlines 3 local

* Регистрация
:PROPERTIES:
:CUSTOM_ID: registration
:END:

Регистрация пользователей на кластере происходит через систему ЦКП
http://ckp.ccfebras.ru.

* Вход в систему
:PROPERTIES:
:CUSTOM_ID: login
:END:

Для работы с системой пользователь должен иметь свою учетную запись на
управляющем узле кластера. Регистрация пользователя на кластере
происходит в соответствии с предыдущей частью руководства. После
регистрации пользователь получает свое имя (логин), пароль и домашнюю
директорию. Если имя пользователя, например, будет *user*, то домашняя
папка находится в */home/user*.

При первом входе в систему предлагается сменить пароль. Требования к
новому паролю: он должен быть достаточной длины, содержать хотя бы 1
цифру и 1 заглавную букву. Директория =~/.ssh= содержит пару ключей
(/id_rsa/ и /authorized_keys/) для доступа к узлам кластера. При
добавлении личного ssh-ключа, необходимо добавить открытый ключ в файл
/authorized_keys/ через перенос строки (т.е. после уже имеющегося одного
открытого ключа).

Пользователи имеют возможность работать на кластере с любой машины,
находящейся в сети института и интернет. Для входа в систему
пользователю необходим адрес сервера ([[http://jupiter.febras.net][jupiter.febras.net]]), а также
имя и пароль, полученные при регистрации.

** Вход с Windows-машины
:PROPERTIES:
:CUSTOM_ID: login_windows
:END:

Работа с системой осуществляется по безопасному протоколу SSH при помощи какого-либо
ssh-клиента. Клиент должен поддерживать протокол версии 2. Рекомендуется использовать 
[[https://www.chiark.greenend.org.uk/~sgtatham/putty/][PuTTY]].
Эта программа является свободно распространяемой и проста в использовании.

После запуска программы (рис. 1) пользователь должен выбрать протокол
ssh и в поле «Host Name (or IP address)» указать адрес
сервера. Нажатие на «Open» приведет к отправке запроса на
подключение. В случае успешного подключения к серверу будет предложено
ввести имя (логин), а затем и пароль.

#+NAME: fig:putty
#+caption: Окно ssh-клиента PuTTY
[[./putty.png]]

При вводе пароля символы на экране не отображаются. Если все введено
правильно, то пользователь автоматически окажется в своей домашней
директории. Этот каталог доступен пользователю с любого узла кластера.

#+begin_note
*Примечание*. На кластере существует единое дисковое пространство для
директорий */opt* (только чтение) и */home*.  Все узлы используют
дисковый массив сервера посредством сетевой файловой системы NFS. Файл
записанный на одном из узлов кластера автоматически становится
доступен на любом другом.
#+end_note

Работа в ssh-сессии происходит в терминальном (текстовом, консольном)
режиме. Необходимо помнить, что консоль Linux, в отличии от Windows,
различает регистр вводимых символов, то есть =mydoc.txt= и =mydoc.TXT= не
одно и то же. После входа на экране отображается консоль командного
интерпретатора в следующем формате имя_пользователя@машина текущий_каталог:

#+begin_src raw
[user@jupiter ~]$
#+end_src

** Вход с терминала Linux
:PROPERTIES:
:CUSTOM_ID: login_linux
:END:

В любой дистрибутив ОС Linux входит терминальный ssh-клиент (обычно
OpenSSH). Минимальный формат команды для подключения к кластеру таков:

#+begin_src raw
[user@localhost ~]$ ssh jupiter.febras.net -l имя_пользователя
#+end_src

# * Копирование файлов

* Работа на кластере
:PROPERTIES:
:CUSTOM_ID: work_cluster
:END:

# ** Навигация

# *** Консоль

# *** mc

# ** Редактирование файлов

# *** mc-edit

# *** Emacs, VIM

** Компиляция программ
:PROPERTIES:
:CUSTOM_ID: compilation
:END:

На кластере (на src_emacs-lisp[:results raw]{(format-time-string "%d.%m.%Y")})
 поддерживаются следующие компиляторы языков
программирования для архитектуры ppc64le:

#+ATTR_HTML: :align center 
#+caption: Компиляторы на кластере
| Компилятор            | Путь к файлу компилятора | Язык       |
| GNU C 4.8.5           | /usr/bin/gcc             | C          |
| GNU C++ 4.8.5         | /usr/bin/g++             | C++        |
| GNU Fortran 4.8.5     | /usr/bin/gfortran        | Fortran 90 |
| IBM XL Fortran 15.1.5 | /usr/bin/xlf             | Fortran 77 |
| IBM XL Fortran 15.1.5 | /usr/bin/xlf90           | Fortran 90 |
| IBM XL C 13.1.5       | /usr/bin/xlc             | C          |
| IBM XL C++ 13.1.5     | /usr/bin/xlc++           | C++        |
| NVIDIA Cuda 8.0.61    | /usr/local/cuda/bin/nvcc | C/C++      |



# *** Замечания по разработке программ на отдельной машине
# :PROPERTIES:
# :CUSTOM_ID: notes_dev
# :END:

** Запуск задач
:PROPERTIES:
:CUSTOM_ID: run_tasks
:END:

*** Диспетчеризация задач
:PROPERTIES:
:CUSTOM_ID: dispatch_tasks
:END:

Для диспетчеризации задач на кластере используется система PBS
Pro. С её помощью пользователь может отправлять свои задачи на
исполнение, снимать их с исполнения и получать информацию по текущему
статусу задачи.

Данная система построена на основе очередей, где под очередью
понимается набор пользовательских процессов (программ, задач)
выполняющихся в рамках системы диспетчеризации. Каждой очереди
сопоставлен ряд атрибутов, в зависимости от которых к задаче будут
применены те или иные действия. Типичными атрибутами являются название
(идентификатор) очереди, её приоритет, доступные ресурсы, количество
задач. В общем случае термин очередь не означает, то что программы в
ней будут выполняться строго последовательно.

Чтобы поставить задачу на исполнение, пользователь должен добавить ее
при помощи команды =qsub= в какую-либо очередь. Очереди отличаются
друг от друга совокупностью ресурсов, которыми они обладают.

*** Система очередей
:PROPERTIES:
:CUSTOM_ID: queues
:END:

На данный момент действует 1 очередь: /workq/.

Для получения информации об очередях, можно выполнить команду =qstat -q=.

#+begin_src raw
[eab@jupiter install]$ qstat -q

server: jupiter1

Queue            Memory CPU Time Walltime Node   Run   Que   Lm  State
---------------- ------ -------- -------- ---- ----- ----- ----  -----
workq              --      --       --     --      0     0   --   E R
                                               ----- -----
                                                   0     0

#+end_src

*Queue* – имя очереди; *Run* – число выполняемых задач; *Que* – число задач, ожидающих начала выполнения.

# Команда =qstat -Qf имя_очереди=  позволяет получить информацию о конкретной очереди.

*** Постановка задачи в очередь
:PROPERTIES:
:CUSTOM_ID: queue_tasks
:END:

Для постановки задачи в очередь на исполнение используется команда =qsub=. Данная команда
принимает в качестве параметра имя скрипта, в котором описываются требуемые задачей ресурсы и
указываются команды, исполняемые при запуске. Рассмотрим пример, иллюстрирующий запуск ранее
скомпилированной программы на 1 "порции" кластера, с использованием 2 mpi процессов на каждой.

#+begin_src raw
[user@jupiter mpi_test]$ cat mpi_test.qsub
#PBS -k oe
#PBS -l select=1:mpiprocs=2
#PBS -r n
#PBS -M user@mail.com
#PBS -m abe
#PBS -q workq
#PBS -N mpi_test
#!/bin/sh

cd /home/user/test/mpi_test

module unload spectrum_mpi && module load openmpi/gcc/1.10.6/4.8.5
mpirun ./mpi

exit 0
[user@jupiter mpi_test]$ qsub mpi_test.qsub
66330.jupiter1
#+end_src

Если команда выполнена успешно, то на экране отобразится идентификатор
задачи (в данном случае это /66330.jupiter1/), в противном случае
появится сообщение об ошибке. Ошибки пользовательской программы
(неправильная компиляция и т.п.) проявятся только при переходе задачи
к активному состоянию.

#+begin_note
*Примечание.* Весь вывод программы в стандартный поток и в поток ошибок
перенаправляется в файлы, находящиеся в домашней директории пользователя.
Названия таких файлов имеют формат =имя_задачи.(e/o)порядковый_номер=. Для
запущенной задачи это будут: /mpi_test.e66330/ – для потока ошибок и
/mpi_test.o66330/ – для стандартного потока вывода.
#+end_note

Прокомментируем каждую из строчек скрипта =mpi_test.qsub=

=#PBS -k oe= — указание сброса потока вывода (o) и потока ошибок (e)

=#PBS -l select=1:mpiprocs=2= — требуемое количество порций (1) и количество mpi процессов на каждой порции (2)

=#PBS -r n= — является ли задача перезапускаемой (задачей с контрольными точками);
*y* — является, *n* — не является

=#PBS -M user@mail.com= — почтовый адрес пользователя

=#PBS -m abe= — какие сообщения отправляются на указанный адрес (*a* — ошибка в
выполнении задачи, *b* — начало выполнения, *e* — завершение
выполнения)

=#PBS -q workq= — идентификатор очереди

=#PBS -N mpi_test= — название задачи

=#!/bin/sh= — указание необходимого командного интерпретатора

=cd /home/user/test/mpi_test= — переход в директорию с исполняемым файлом

=module unload spectrum_mpi && module load openmpi/gcc/1.10.6/4.8.5= — выбор openmpi вместо spectrum_mpi

=mpirun ./mpi= — запуск приложения

=exit 0= — выход

# *** Запуск интерактивных программ
# :PROPERTIES:
# :CUSTOM_ID: interactive
# :END:

# *** Запуск непараллельных программ

# *** Состояние пользовательских задач

*** Остановка задач
:PROPERTIES:
:CUSTOM_ID: stop_tasks
:END:

Остановка программы производится командой =qdel идентификатор_задачи=
#+begin_src raw
[user@jupiter ~]$ qdel 700
#+end_src

Этой командой задача, стоящая в очереди, убирается из нее, а
выполняющаяся задача снимается с выполнения. Следующая по очереди и
приоритету задача встает на выполнение.

Задача снимается в течении некоторого времени, поэтому при вызове
=qstat= непосредственно после =qdel= удаленная задача все еще может
быть отражена в таблице.

* Мониторинг
:PROPERTIES:
:CUSTOM_ID: monitoring
:END:

** Web-интерфейс
:PROPERTIES:
:CUSTOM_ID: web_interface
:END:

Мониторинг кластера реализован при помощи системы Ganglia. Эта система позволяет следить
за ресурсами кластера посредством web-интерфейса. Система мониторинга находится по адресу 
http://jupiter.febras.net/ganglia.

Для мониторинга пользователю доступно большое число типов ресурсов:
загруженность процессора, оперативная память, загрузка сети, средняя
загрузка, количество процессов и ряд других.  Имеется возможность
наблюдать как за всеми узлами в кластере (по одному параметру), так и
за каждым (по всем параметрам).


** Консоль
:PROPERTIES:
:CUSTOM_ID: console
:END:

#+begin_src raw
[user@jupiter ~]$ pbsnodes jupiter2
jupiter2
     Mom = jupiter2
     Port = 15002
     pbs_version = 14.1.0
     ntype = PBS
     state = free
     pcpus = 160
     resources_available.arch = linux
     resources_available.host = jupiter2
     resources_available.mem = 263653568kb
     resources_available.ncpus = 160
     resources_available.ngpus = 2
     resources_available.vnode = jupiter2
     resources_assigned.accelerator_memory = 0kb
     resources_assigned.mem = 0kb
     resources_assigned.naccelerators = 0
     resources_assigned.ncpus = 0
     resources_assigned.netwins = 0
     resources_assigned.ngpus = 0
     resources_assigned.vmem = 0kb
     resv_enable = True
     sharing = default_shared

#+end_src

* Справочная информация
:PROPERTIES:
:CUSTOM_ID: reference
:END:

Описание основных команд при работе в ОС Linux – http://wwwinfo.jinr.ru/unixinfo/pc/lin_os.html
Документация к системе диспетчеризации заданий PBS Pro — http://www.pbsworks.com/pdfs/PBSUserGuide14.2.pdf
