#+TITLE: Архитектура системы организации программного обеспечения кластера
#+OPTIONS: toc:t H:6 num:t html-postamble:nil ^:nil tags:nil author:nil
#+SETUPFILE: theme-readtheorg-local.setup
#+HTML_HEAD: <style type="text/css">.org-src-name{ text-align: right; }</style>
#+HTML_HEAD: <style type="text/css">.outline-2{ margin-top: 60px; }</style>
#+HTML_HEAD: <style> p {text-align: justify; } </style>
#+MACRO: fa @@html:<i class="fa fa-$1"></i>@@

#+begin_quote
Процесс построения системы является основой для её *расширения* (способности к изменениям).
#+end_quote

- Выстроить репозитории, из которых будет вестись построение системы.
- Заморозить версии пакетов.
- Продумать переезд на новую конфигурацию кластера.
- Разрабатывать инфраструктуру таким образом, чтобы процесс реорганизации кластера протекал безболезненно.

#+begin_quote
Необходимо, чтобы система *времени выполнения* была способна к реорганизации.
#+end_quote


_В чем проблема при разработке расширения PBS и Environment_?

Нет четко определённых мест, куда бы укладывались коммиты, полностью описывающие добавляемую "фичу", реализующую требуемую функциональность. Не должно быть ощущения, что система сложная и непонятно, куда и что добавлять.

#+begin_note
Если программа устанавливается в =/opt=, а потом её системные файлы "патчатся" с помощью =pbsrun_wrap=, как это представлять? Можно ли для этого использовать Environment Modules? Это говорит о непродуманном процессе *построения* системы: все постепенно превращается в "снежинку" (монолит).
#+end_note

Возможно, надо рассмотреть декомпозицию системы на слои, построение каждого из которых осуществляется по одному шаблону, например:

- Слой RPM
  - Собирается система замороженных версий пакетов, в т.ч. драйверов. Зависимости удовлетворяются.

- Слой конфигурационных файлов
  - Собирается система конфигов (при этом, могут понадобиться действия времени выполнения: перезапуск служб и т.п.). Зависимости могут быть описаны глобальными переменными Ansible.

Вообще говоря, это слой модифицирует состояние конфигов, которое было получено при базовой установке слоя RPM пакетов. Кроме того, возможно вертикальное разделение по ролям Ansible.

Что насчёт конфигурации сервера PBS? (это наиболее вероятный объект для внесения изменений, в том числе и новой версии RPM пакета).

- Слой "несистемных" =/opt=, контейнеры
  - Ожидает, что будет конкретная конфигурация предыдущих слоёв, на которые он опирается.

После того, как система собирается из списка RPM пакетов, формируется множество системных и конфигурационных файлов $F_0$. Если ничего не нарушено, при воспроизведении системы будем получать одно и то же множество файлов. Далее, следует этап конфигурирования, получаем версию $F_1$. Теперь наступает этап, когда надо _интенсивно_ разрабатывать систему, базирующуюся на $F_1$. По замыслу, это уже нечто другое, чем просто конфигурирование системы - это разработка функционала, опирающегося на $F_1$. PBS, mpirun, environment modules, всё, что будет затронуто при разработке.

Необходимо зафиксировать _окончание_ построения *базовой системы*. Такие вещи, как версии драйверов, пакетов, смена типа драйвера или подобные задачи возможны, но это будет просто сопровождение базовой системы. В ней присутствует "концептуальная" завершённость. Она предоставляет видимый "периметр", внешний API, базовый набор пакетов и минимально необходимые настройки конфигурации.

Далее, возможно сильное изменение базовой функциональности исходя из потребностей разработки. Как организовать сшивку этих 2х систем? Как-то хрупко выглядит использование таких вещей, как =pbsrun_wrap=, которые меняют базовую систему. Вместо этого надо осознанно провести границу между ними и следить за обеспечением _целостности_ базовой системы.

Значит, например, обёртки необходимо представить по-другому:

- Слой PBS, mpirun, env. mod., UI
  - Средствами environment modules перекрыть необходимые имена mpirun, mpiexec так, чтобы это произошло безболезненно для базовой системы.

Более широко: надо извлечь из базовой системы то, что по смыслу более ближе к этому следующему слою. Это похоже на User Space: слой, относящийся к пользовательскому интерфейсу. Необходимо выстроить связующий уровень: при построении этого слоя он опирается на предыдущий.

Всё, что предоставляет базовый уровень, можно описать исходя из его построения.

На что похожа эта _система управления_? (... "все самописное" ...)

Можно ли её воплощать в production и test instances? Это зависит от того, какой "API" предоставляет базовая система. С помощью контейнеров можно безболезненно запустить инстанс базовой тестовой системы в обход продакшен, при этом можно регулировать: какая часть базовой системы видна в контейнере тестовой.

Как проводить тестирование? Как воспроизвести окружение пользователя и запускать из него, например, тестовые job'ы с подключением модулей?

Какие еще есть интересные незатронутые вопросы?

Как будет развертываться вторая (система управления) система? Самый простой вариант - тем же ансиблом, поверх базовой. Должны ли какие-то пакеты устанавливаться в ней? На примере PBS: пропатченная версия ведь относится к этой второй системе больше? Установка пропатченной версии - это "эпизод из жизни" именно второй системы.

#+begin_note
Необходимо так провести разделение на 2 части, чтобы обе системы только выиграли от этого.
#+end_note

_Разделение системы на части_

Выделение ещё одного слоя. Описание архитектуры.

1) Очевидно, сейчас работа с Environment Modules организована некомфортно (в т.ч. и в ansible), её надо перенести в новую систему и организовать лучше.
2) bashrc тоже в новую, сложности с if'ами уйдут из первой системы

#+begin_note
Такое вырезание части первой системы - болезненная процедура, поскольку она может повлиять на целостность.
#+end_note

Интуитивно всё время есть ощущение, что построение системы заново на базе предыдущего опыта (проведение следующей итерации) /значительно/ проще, комфортнее, предсказуемее, надёжнее, чем попытка реорганизовать имеющуюся систему. О чем это говорит?

Во-первых, о том, что способность к плавной реорганизации не закладывалась при построении системы. Хотя бы частичная перестройка частей системы, которая была бы безболезненной для системы в целом (способность к расширению?).

Во-вторых, это говорит о том, что при такой итерации действительно достигаются многие возможности, недоступные при реорганизации. (Банально: то, что система выполняется беспрерывно в реальном времени - мешает реорганизации. Напротив, построение системы с нуля даёт полную свободу. Но если думать в терминах переноса сервисов (похоже, это есть процесс, как говорится в "Красивых Архитектурах"), как будет замечено далее, возможен перенос их по очереди с сохранением работоспособности всей системы.)

#+begin_note
Не факт, что на следующей итерации получится построить более лучшую систему: можно допустить другие ошибки и получить "перепроектированную" систему.
#+end_note

С другой стороны: смотреть, как сделано сейчас, и улучшать везде, где можно, при построении системы на второй итерации - это вполне адекватный способ получения лучшей системы. Кроме того, надо заморозить часть системы (запретить измения): тогда это дает гарантию, что она обладает всеми нужными свойствами: предсказумость, целостность и т.п. Но не полностью запретить изменения, а выделить пути расширения, не влияющие на целостность: пополнение списка пакетов по определенной процедуре, накопление изменения конфигурации в специально отведенном месте (которое точно, рано или поздно, попадет в коммит) и т.п.

Ещё ведь есть 3я система: =/opt=, "система сборки". В какой мере работоспособность собранных программ в =/opt= зависит от состояния базовой системы? Т.е. она также опирается на базовую систему, как и вторая система. Также справедливо соображение о соблюдении целостности базовой системы: смешивание в =/opt= элементов базовой системы и системы сборки вности лишний беспорядок.

TODO Необходимо разделить =/opt=

Надо вспомнить, как мимоходом было принято решение об устройстве =/opt= - стихийно, не ожидая, что это приведет к такого рода беспорядку.

Что именно мешает ментальному восприятию базовой системы, как целостной, воспроизводимой, предсказуемой и т.п.? А такие мелочи и мешают. Слишком многое остается "на потом".

_Построение системы с помощью ролей Ansible_

_Роли_ представляют из себя те самые *структуры*, с помощью которых осуществляется декомпозиция системы.

Внесение изменений и сопровождение осуществляется в терминах ролей.

#+begin_note
Вносить изменения только в специально отведенных местах, которые прямо или косвенно находятся под контролем версий.
#+end_note

Нужно иметь перед глазами "общесистемный" коммит, который описывает то или иное изменение в системе. Кроме того, сложность представляют сами шаги, которые необходимо выполнить, чтобы внести этот коммит в систему (перезапуск служб и т.д.) Кроме того, не всегда бывает возможность сразу реализовать роль Ansible: иногда изменения накапливаются во временном месте (wiki, и это тоже должно входить в "общесистемный" коммит).

_Стихийная система_

Такое ощущение, что выстраивание логичной системы очень затруднительно после того, как она уже стихийно выстроена. Если сейчас начать заново развёртывать всю систему - есть ощущение, что получится сделать значительно лучше. А при попытках мысленно перестроить её - такого ощущения нет. Причиной этого, вероятно, является то обстоятельство, что факт развёртывания является важным тестом, позволяющим убедиться в отстутствии в системе скрытых мест (не попадающих в коммиты, стихийно добавленных изменений), которые могут впоследствии проявиться (если развертывание происходит редко).

#+begin_note
Как сделать, чтобы управляющий узел можно было перезагружать наравне с остальными узлами. PBS можно продублировать.
#+end_note

Всё равно, конфиг будет распределен между У.У. и В.У.

Надо мыслить в терминах "красивой архитектуры", хорошо понять то, что есть сейчас, разпланировать построение и сопровождение системы.

Поможет ли это в понимании того, как организовать обёртки mpirun? А подсистема Environment Modules?


_Связь между системой сборки и системой управления_

Очевидно, требуется добавлять модули для собранного ПО и прочие подобные зависимости.

#+begin_center
#+begin_src dot :file ./schema.png
digraph system {
subgraph system {
    node[shape=Mrecord];
    system [label="{<user> система\nуправления| {<build> система\nсборки|<base>базовая\nсистема}}"];
}
subgraph commit {
    node[shape=Mrecord];
    commit [label="{doc\nwiki | { com }| { ... }| { com }} | {repo\ngitlab | { mit }| { ... }| { mit }} "];
}
}
// digraph packages {
//         compound=true;
// 	size="30,40";
// 	"base" [label = "Базовая система"; shape=box];
// 	"build" [label = "Система сборки"; shape=box];
// 	"user" [label = "Расширение базовой"; shape=box];
// 	"base" -> "build" [dir="both"];
// 	"base" -> "user" [dir="both"];
// 	"build" -> "user" [dir="both"];
// }
#+end_src
#+end_center


_Цели очередной итерации базовой системы:_

- Заморозить версии пакетов
- Восстановление БД из бекапа, перенос текущих БД
- Достичь целостности системы
- Создать описание системы на основе её построения (документация)
- Внести мелкие улучшения (repos, общее расположение (композиция), роли, ...)
- Внедрить репозиторий "наших" пакетов (связь со "сборкой")
- Отделить часть, относящуюся к "Управлению"
- Возможно отделить часть, относящуюся к "Сборке"
- Разобраться с установкой драйверов NVIDIA
- Определить (изолированные) места в системе, которые могут предоставить возможности для расширения, не нарушая при этом целостность - т.е. если админы добавляют что-то в правильное место предсказуемым способом - оно попадает в коммит (репозиторий)
- Тесты?
- Создание репозитория в gitlab
- Горизонтальный коммит: документация (в т.ч. ссылки на gitlab) + код (ansible и т.п.)

_Цели создания системы сборки_

- Предоставление репозитория "наших" пакетов
- Получение следующего свойства сборки ПО: доведение действий по сборке до определённого состояния (возможно, заход в тупик) и передача другому специалисту с тем, чтобы он мог в точности с того самого состояния начать "ковыряться" с пакетом (воспроизводимость)
- Для тех пакетов, для которых может потребоваться многократная пересборка (с улучшенным набором параметров, например) это единственный способ экономить усилия
- Может потребоваться пересборка спустя долгое время, когда многие детали уже забылись
- Обкатка "уровней документирования":
  - высокая степень автоматизации, краткое описание
  - минимум автоматизации, детальное описание ручных шагов, которые необходимо выполнить
- Обобщение сборок различных типов RPM, =make install=, container
  - применение тех же приёмов автоматизации и документирования
- Тесты? Обеспечить видимые результаты выполнения тестов (поставляемых с пакетом) в системе сборке

_Цели создания системы управления_

- Удобство использования всей системы в целом основывается на качественном пользовательском интерфейсе
- Разобраться с нарастающим хаосом в Environment Modules
- Достичь уровня интеграции компонент, сравнимого с самописной системой (больше похоже на разработку)
- Обеспечить безболезненное расширение базовой системы путем расположения элементов и структур поверх "базово системных"
  - например, обёртки mpirun
- Достичь уровня тестируемости системы "пользователь", сравнимого с системой тестов в программных проектах
- Использовать все преимущества старта нового проекта с точки зрения организации
- Gitlab repo
- Тестовая система? Production and test instances

_Как обеспечить интеграцию систем_

Архитектурный стиль: использование репозиториев для отслеживания изменений, частичное перетекание из документации в автоматизацию, использование паттерна XaaC для документации, базовой системы, программного проекта "Пользователь", системы сборки; применение приёмов работы с кодом для обеспечения нужных свойств системы (добавление коммитов слоями).

Это скорее концептуальная целостность, чем интеграция!


#+begin_note
Для обеспечения целостности необходимо применять сокрытие.
#+end_note

Собственно, особой интеграции не требуется, т.к. системы по определению используют элементы нижележащих систем.

Возможно, следует проследить то, что нижележащая система предоставляет верхним? Хотя, наверное, это "предоставляет всё, что может" (главное, это целостность нижележащей системы)

Интеграция, скорее, необходима с точки зрения _кода_ (метаинтеграция). Например, возможно потребуется использовать нечто вроде глобальных переменных Ansible.

TODO документируемость контейнеров
Описать регламент добавления в систему Сборки элементов (воспроизводимость, сопровождение)

Зафиксировать базовую систему: с какой точки зрения? Что добавлять в неё больше не стоит (количественные, т.е. добавление, расширение списка значений уже существующих типов - можно, а создавать новые типы - нельзя)

#+begin_note
Места для расширения _другими админами_ (роли), которые гарантируют попадание в репозиторий.
#+end_note

Получается, надо строить систему ещё и вокруг Ролей специалистов. Например, Роль "способный вести репозитории".

#+begin_note
А не нужно ли изолировать БД от базовой системы? Надо интуитивно понять, как вести себя с БД: LDAP, MySQL, PostreSQL
#+end_note

Например, =pbstools= при установке должен _интегрироваться_ с *базовой системой*, принадлежа при этом системе сборки. Это же касается =/opt=, =soft= и т.д.

#+begin_note
Добавление пакетов - механизм расширения базовой системы.
#+end_note

Добавление =/opt=, =soft=, =pbstools= - по той же причине, что и смешивание в =/opt=, плохо и смешивание в =/etc=.
Если что-то _смешивается_ с базовой системой, оно размазывается по нескольким структурам вместо того, чтобы аккуратно "прописаться" в одной и задействовать те или иные механизмы интеграции. Например, роли Ansible позволяют контролировать состояние файлов из =/etc=.

На примере базовой системы продемонстрировать общий _архитектурный стиль_, который рекомендуется применять для остальных подсистем.

Организовать материал так, чтобы замечания, относящиеся к другим подсистемам, органически дополняли пример базовой системы.

_Сокрытие_ - для базовой системы, которое сейчас не работает, но можно писать о нём будто для следующей итерации (наряду с заморозкой версий rpm).

Как планируется строить *систему управления*?

По ходу описания необходимо приводить примеры неудачных решений.

Исходя из опыта можно сказать, что всегда есть функционал, который надо быстро (с минимальными размышлениями о том, как аккуратно интегрировать его в имеющуюся систему) - воткнуть. Значит, должно изначально проектироваться _место_ для подобных добавок, не гарантирующее, впрочем, что при следующей итерации воспроизведения системы все заработает (ничего не отвалится).

Как обеспечить воспроизводимость _системы управления_? Должен существовать механизм, позволяющий "обнулять" её, т.е. приводить всю систему к такому состоянию, что есть только базовая. А затем накатывать систему управления.

Цель: обеспечить безболезненное воспроизведение всей системы с нуля с минимальными потерями. Потери могут произойти, если в процессе расширения системы значимая функциональность не попала в конфигурацию и не воспроизвелась в новой системе, оставшись только в старой.

Поэтому, надо провести четкую границу между базовой системой и "расширениями", которые будет необходимо переносить в новую.

#+begin_quote
Имеет изолированные элементы, что сводит к минимуму количество мест внесения изменений при модификации.
#+end_quote

Один аспект HA (High Availability) - это способность системы пережить переезд в следующую итерацию. Для БД надо делать backup и restore. Для портов надо делать перенаправление. Для хранилища надо делать переподключение.

Для базовой системы необходимо отработать процесс перехода в следующую итерацию. RAID1 гарантирует сохранность самих БД. 

#+begin_note
Нужно развертывать систему так, как если бы БД уже восстанавливались из бэкапа.
#+end_note

_Связь между системой сборки и базовой системой_

Для компиляции софта требуется наличие пакетов в системе. Некоторые могут ставиться через rpm, некоторые через =make install=. Если система сборки опирается на пакеты из =/opt=, то получается сложная зависимость - =/opt= становится частью системы сборки.

На что похожа "система сборки"?

Возможно, стоит заняться ей, также разделить их на две части (их - т.е. вместе с базовой системой), чтобы они обе только выиграли от этого.

Чтобы сформулировать то, как система сборки опирается на базовую систему, необходимо в точности понимать, что из себя представляет базовая система. Что в ней должно быть, а чего - не должно. (Например, не должно быть обёрток, смешивания в =/opt=, =/etc/=, прямых модификаций файлов и каталогов базовой системы). Всё, что относится к такому ручному вмешательству, должно быть выделено в другую систему. =pbstools= можно отнести к _системе управления_, т.к. это логически относится к ней. Кроме этого, были мелкие расшаривания каталогов для syler.

Чтобы понять, как организовать *систему сборки*, необходимо перечислить приемы, которые имеются в арсенале. Назвать задачи, которые должна решать система сборки.

Гибкость (легкость внесения изменений) - переконфигурация кластера на новые цели, исходя из финансовых соображений, например. Это может потребовать существенных изменений в частных характеристиках (что именно поддерживается, выбор пакетов в базовой системе, конфигурация, на что ориентирован продукт и т.д.), но архитектура должна уметь переживать подобную переориентацию.

А не придется ли строить систему вокруг конкретных людей (ролей)?

Это проблема, что список поддерживаемых пакетов будет увеличиваться в длине и становится большим?

Относятся ли архитектурному стилю "уровни документирования"?

Получается, что частота воспроизведения базовой системы - очень низкая. Вполне возможно, что после развёртывания aux узла, она будет зафиксирована очень надолго. Значит, она должна уметь переживать необходимые "расширения", быть очень аккуратной, прозрачной, компактной и т.д.

Это какой-то общий принцип: так организовать ядро системы, чтобы оно было компактным и т.д.

Это и преимущество базовой системы, что она редко меняется.

Какие выделить "места" для установки в систему, не затрагивающие базовую?

Кстати! Теперь ведь будут 2 ОС CentOS - для управляющего и вычислительных узлов. Но та часть функциональности, которая связана с системой сборки, сейчас привязана к смешанному узлу. Если она будет привязана к вычислительному узлу, это позволит еще более упростить конфигурацию.

Теперь тонкий момент: как именно остальные системы будут опираться на базовую, если она так разделена?

Хорошая возможность после накопления опыта сделать все правильно на следующей итерации. Кроме того, понимание того, что есть еще 2 системы: система сборки и система управления, позволит ещё более продуманно выстроить базовую систему.

Можно ли, используя нынешний xCAT + Ansible, экспериментировать с построением aux узла (raid1, убрать postinstall)? В этом случае, можно получить bootstrap, когда для того, чтобы начать развёртывать систему достаточно иметь xCAT и Ansible. А эта связка теперь всегда будет в наличии, т.к. первым делом в новую систему будет установлен xCAT.

Что произойдет, когда надо будет использовать конфигурацию Ansible для поддержки сразу двух систем: нынешнего jupiter и aux - будущей второй итерации? Что это вообще _качественно_ такое для конфигурации - способность "работать" на несколько систем? Возможно, это важный момент, - иметь систему, которая будучи однажды развёрнута, затем бутстрапится для следующих систем. При этом, надо добиться того, чтобы сохранялась "преемственность" с прежней системой.
(Это достигается с помощью _автоматического распространения_).

Надо думать в этом направлении. Такие шаги:
1) развернуть второй головной узел
2) убедиться в их идентичности с первым
3) переподключить физические коннекты, при необходимости
4) довести новый головной узел "на месте"
5) вывести старый головной узел в подчинение

Возможно, надо заморозить и версию xCAT тоже? Не годится, что может глюкануть из-за его обновления до другой версии в следующей итерации.

Единственно, что неудобно для такого bootstrapping - это смена DNS, DHCP в сети. Ведь, чтобы проверить работоспособность второй системы надо либо выделять её в отдельную подсеть, либо останавливать сервисы DNS, DHCP на первой.

Можно изолировать xCAT (вместе с DNS, DHCP), иметь уверенность в работоспособности его при переносе и использовать xCAT со старой системы. Тогда в новую он будет устанавливаться в выключенном состоянии.

#+begin_note
В дополнении к списке БД еще БД xCAT.
#+end_note

Тогда эти 2 системы будут идти с некоторой дельтой, связанной с неактивными на данный момент настройками. К этой дельте будет прилагаться инструкция, по которой можно вывести первую систему и ввести вторую. Так можно получить реально работоспособную систему, а не "приблизительно".

#+begin_quote
Во времена *p8* была попытка создать головной узел из xCAT, но получилась проблема с RAID1 и kickstart? Или что было? Уже не помню. Возможно, было сомнение в скриптах postinstall xCAT.
#+end_quote

_План перевода на aux узел_

Ввести оба узла, отличающиеся критическими сервисами, которые могут быть только в одном экземпляре в локальные сети. Это будет этап подготовки к переносу сервисов. Затем, можно переносить сервисы по очереди:
PBS, LDAP, MySQL, Ganglia, xCAT (DNS, DHCP), ...

После этого, можно оставить бывший головной узел в качестве вычислительного с сохранением всех накопленных настроек с тем, чтобы при необходимости можно было посмотреть и вспомнить незамеченные настройки.

_Общее правило переноса версисов_: подготовить базовую систему, максимально приблизить её к текущей, затем перенести сервисы вручную.

Для того, чтобы проворачивать такие фокусы, нужно иметь конфигурацию Ansible очень качественно организованную.

#+begin_note
Надо с самого начала резервировать имена, адреса, конфиги для двух избыточных систем, чтобы потом сменой настройки перенаправить клиентов на новые сервера. Ganglia можно перенаправить простым перенаправлением порта на адрес во внутренней сети. По-хорошему, внешний интерфейс (22 ssh, 80 web) надо иметь независимым, чтобы можно было перенаправлять порты на сервисы во внутренней сети. С другой стороны, можно ввести новый узел в строй, перенаправив его внешние порты на старый узел - тогда все будет работать, как прежде.
#+end_note

Возможно, стоит отработать все эти идеи на тестовой системе, сконцентрироваться именно на xCAT, DNS, DHCP, PXE и настройках primary, secondary. Как клиентские системы привязваются к xCAT? Где должны располагаться файлы и каталоги небазовых систем?

Конфигурация должна поддерживать создание дублирующей системы также органично, как и основной.

